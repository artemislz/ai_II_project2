{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a80865b",
   "metadata": {
    "papermill": {
     "duration": 0.009187,
     "end_time": "2025-05-04T13:58:57.437120",
     "exception": false,
     "start_time": "2025-05-04T13:58:57.427933",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d083b644",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T13:58:57.453633Z",
     "iopub.status.busy": "2025-05-04T13:58:57.453139Z",
     "iopub.status.idle": "2025-05-04T13:59:01.821292Z",
     "shell.execute_reply": "2025-05-04T13:59:01.820598Z"
    },
    "papermill": {
     "duration": 4.377561,
     "end_time": "2025-05-04T13:59:01.822758",
     "exception": false,
     "start_time": "2025-05-04T13:58:57.445197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q ftfy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d701bd",
   "metadata": {
    "papermill": {
     "duration": 0.007164,
     "end_time": "2025-05-04T13:59:01.837536",
     "exception": false,
     "start_time": "2025-05-04T13:59:01.830372",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Seeds - Determinism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b19d481",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T13:59:01.852867Z",
     "iopub.status.busy": "2025-05-04T13:59:01.852625Z",
     "iopub.status.idle": "2025-05-04T13:59:06.467050Z",
     "shell.execute_reply": "2025-05-04T13:59:06.466267Z"
    },
    "papermill": {
     "duration": 4.623586,
     "end_time": "2025-05-04T13:59:06.468300",
     "exception": false,
     "start_time": "2025-05-04T13:59:01.844714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\" \n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a80e9b1",
   "metadata": {
    "papermill": {
     "duration": 0.007273,
     "end_time": "2025-05-04T13:59:06.483390",
     "exception": false,
     "start_time": "2025-05-04T13:59:06.476117",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c742d7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T13:59:06.499117Z",
     "iopub.status.busy": "2025-05-04T13:59:06.498816Z",
     "iopub.status.idle": "2025-05-04T13:59:45.468384Z",
     "shell.execute_reply": "2025-05-04T13:59:45.467780Z"
    },
    "papermill": {
     "duration": 38.978936,
     "end_time": "2025-05-04T13:59:45.469821",
     "exception": false,
     "start_time": "2025-05-04T13:59:06.490885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random, re, os\n",
    "import ftfy\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from gensim.models import KeyedVectors\n",
    "import optuna\n",
    "import torch.nn.functional as F\n",
    "from optuna.exceptions import TrialPruned\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gensim.models import Word2Vec\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a238a500",
   "metadata": {
    "papermill": {
     "duration": 0.007434,
     "end_time": "2025-05-04T13:59:45.485198",
     "exception": false,
     "start_time": "2025-05-04T13:59:45.477764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# From Tweet to Vector Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "008fe42c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T13:59:45.501270Z",
     "iopub.status.busy": "2025-05-04T13:59:45.500503Z",
     "iopub.status.idle": "2025-05-04T13:59:45.505585Z",
     "shell.execute_reply": "2025-05-04T13:59:45.505042Z"
    },
    "papermill": {
     "duration": 0.014021,
     "end_time": "2025-05-04T13:59:45.506572",
     "exception": false,
     "start_time": "2025-05-04T13:59:45.492551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "\n",
    "def get_tweet_vector(tokens, w2v_model, embedding_dim, agg=\"mean\", seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if token in w2v_model.wv:\n",
    "            vectors.append(w2v_model.wv[token])\n",
    "    \n",
    "    if not vectors:\n",
    "        # If no tokens found in the model, return a random vector\n",
    "        return np.random.normal(0, 1, embedding_dim)\n",
    "    \n",
    "    vectors = np.array(vectors)\n",
    "    \n",
    "    if agg == \"mean\":\n",
    "        return vectors.mean(axis=0)\n",
    "    elif agg == \"sum\":\n",
    "        return vectors.sum(axis=0)\n",
    "    elif agg == \"max\":\n",
    "        return vectors.max(axis=0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown aggregation method: {agg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b42dd4",
   "metadata": {
    "papermill": {
     "duration": 0.006969,
     "end_time": "2025-05-04T13:59:45.520908",
     "exception": false,
     "start_time": "2025-05-04T13:59:45.513939",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95d610f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T13:59:45.536186Z",
     "iopub.status.busy": "2025-05-04T13:59:45.535646Z",
     "iopub.status.idle": "2025-05-04T13:59:45.540095Z",
     "shell.execute_reply": "2025-05-04T13:59:45.539588Z"
    },
    "papermill": {
     "duration": 0.013163,
     "end_time": "2025-05-04T13:59:45.541107",
     "exception": false,
     "start_time": "2025-05-04T13:59:45.527944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaselineNet(nn.Module):\n",
    "    def __init__(self, D_in=EMBEDDING_DIM, H1=128, H2=64, H3=32):\n",
    "        super(BaselineNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in, H1)\n",
    "        self.linear2 = nn.Linear(H1, H2)\n",
    "        self.linear3 = nn.Linear(H2, H3)\n",
    "        self.linear4 = nn.Linear(H3, 1) \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.relu(self.linear3(x))\n",
    "        return self.linear4(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a484da90",
   "metadata": {
    "papermill": {
     "duration": 0.007129,
     "end_time": "2025-05-04T13:59:45.555421",
     "exception": false,
     "start_time": "2025-05-04T13:59:45.548292",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93bd649",
   "metadata": {
    "papermill": {
     "duration": 0.007026,
     "end_time": "2025-05-04T13:59:45.569654",
     "exception": false,
     "start_time": "2025-05-04T13:59:45.562628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Help Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efe718be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T13:59:45.585068Z",
     "iopub.status.busy": "2025-05-04T13:59:45.584832Z",
     "iopub.status.idle": "2025-05-04T13:59:45.601985Z",
     "shell.execute_reply": "2025-05-04T13:59:45.601524Z"
    },
    "papermill": {
     "duration": 0.026138,
     "end_time": "2025-05-04T13:59:45.603009",
     "exception": false,
     "start_time": "2025-05-04T13:59:45.576871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "contractions_dict = {\n",
    "    \"dont\": \"do not\", \"doesnt\": \"does not\", \"didnt\": \"did not\",\n",
    "    \"cant\": \"can not\", \"wont\": \"will not\", \"shouldnt\": \"should not\",\n",
    "    \"couldnt\": \"could not\", \"isnt\": \"is not\", \"arent\": \"are not\",\n",
    "    \"hasnt\": \"has not\", \"havent\": \"have not\", \"hadnt\": \"had not\",\n",
    "    \"wouldnt\": \"would not\", \"mustnt\": \"must not\", \"wasnt\": \"was not\",\n",
    "    \"werent\": \"were not\", \"mightnt\": \"might not\", \"shant\": \"shall not\",\n",
    "\n",
    "    \"im\": \"i am\", \"youre\": \"you are\", \"ure\":\"you are\", \"hes\": \"he is\", \"shes\": \"she is\",\n",
    "    \"its\": \"it is\", \"were\": \"we are\", \"theyre\": \"they are\",\n",
    "    \"thats\": \"that is\", \"whats\": \"what is\", \"wheres\": \"where is\",\n",
    "    \"whos\": \"who is\", \"hows\": \"how is\", \"heres\": \"here is\",\n",
    "    \"theres\": \"there is\",\n",
    "\n",
    "    \"ive\": \"i have\", \"youve\": \"you have\", \"uve\":\"you have\",\"weve\": \"we have\", \"theyve\": \"they have\",\n",
    "    \"whove\": \"who have\", \"whatve\": \"what have\", \"whereve\": \"where have\",\n",
    "    \"howve\": \"how have\",\n",
    "\n",
    "    \"ill\": \"i will\", \"youll\": \"you will\", \"ull\":\"you will\", \"hell\": \"he will\",\n",
    "    \"shell\": \"she will\", \"itll\": \"it will\", \"well\": \"we will\",\n",
    "    \"theyll\": \"they will\", \"wholl\": \"who will\", \"whatll\": \"what will\",\n",
    "    \"wherell\": \"where will\", \"howll\": \"how will\",\n",
    "\n",
    "    \"id\": \"i would\", \"youd\": \"you would\", \"ud\":\"you would\",\"hed\": \"he would\",\n",
    "    \"shed\": \"she would\", \"itd\": \"it would\", \"wed\": \"we would\",\n",
    "    \"theyd\": \"they would\", \"whod\": \"who would\", \"whatd\": \"what would\",\n",
    "    \"whered\": \"where would\", \"howd\": \"how would\",\n",
    "\n",
    "    \"gimme\": \"give me\", \"gonna\": \"going to\", \"gota\": \"got to\",\n",
    "    \"lemme\": \"let me\", \"wanna\": \"want to\", \"hafta\": \"have to\",\n",
    "     \"dunno\": \"do not know\", \"yall\": \"you all\", \"cmon\": \"come on\",\n",
    "     \"aint\": \"is not\"\n",
    "}\n",
    "\n",
    "\n",
    "def replace_mentions(text):\n",
    "    \"\"\"Replaces @mentions with the keyword 'username', even if after punctuation.\"\"\"\n",
    "    return  re.sub(r'\\@\\w+', ' username ', text)\n",
    "\n",
    "def replace_emoticon(text):\n",
    "    text = re.sub(r\"<3\", \" love \", text)\n",
    "    text = re.sub(r\"<33\", \" love \", text)\n",
    "    text = re.sub(r\"</33\", \" heartbroken \", text)\n",
    "    text = re.sub(r\"</3\", \" heartbroken \", text)\n",
    "    text = re.sub(r\";\\)\", \" wink \", text)\n",
    "    text = re.sub(r\";-\\)\", \" playful \", text)\n",
    "    text = re.sub(r\":-d\", \" laugh \", text)\n",
    "    text = re.sub(r\"\\(:\", \" smile \", text)\n",
    "    text = re.sub(r\":P\", \" playful \", text)\n",
    "    text = re.sub(r\":\\*\", \" kiss \", text)\n",
    "    text = re.sub(r\":'\\(\", \" sad \", text)\n",
    "    text = re.sub(r\":\\|\", \" neutral \", text)\n",
    "    text = re.sub(r\" :o \", \" wow \", text)\n",
    "    text = re.sub(r\"&\", \" and \", text)\n",
    "    text = text.replace(\"♥\", \" heart \")\n",
    "    text = text.replace(\"♫\", \" music \")\n",
    "    text = text.replace(\"☺\", \" smile \")\n",
    "    text = re.sub(r\"\\bw\\/out\\b\", \" without \", text)\n",
    "    text = re.sub(r\"\\bw\\/o\\b\", \" without \", text)\n",
    "    text = re.sub(r\"\\bw\\/(?=\\s|\\w)\", \" with \", text)\n",
    "    text = re.sub(r\"\\bw\\b\", \" with \", text)\n",
    "    text = re.sub(r\"\\bb\\/c\\b\", \" because \", text)\n",
    "    text = re.sub(r\"\\bh\\/w\\b\", \" homework \", text)\n",
    "    text = re.sub(r\"\\bno\\s*-\\s*one\\b\", \" noone \", text)\n",
    "    text = re.sub(r\"\\bb\\s*-\\s*day\\b\", \" birthday \", text)\n",
    "    text = re.sub(r\"\\bhell\\b\", \" xhellx \", text)\n",
    "    return text\n",
    "\n",
    "def remove_specific_punctuation(text):\n",
    "    return re.sub(r\"[\\'@*]\", \"\", text)\n",
    "\n",
    "def replace_punctuation_with_space(text):\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "    text = re.sub(r\"[_]\", \" \", text)  # Replace underscore with space\n",
    "    return text\n",
    "\n",
    "def fix_mojibake(text):\n",
    "    return ftfy.fix_text(text)\n",
    "\n",
    "slang_dict = {\n",
    "    \"luv\": \"love\",\n",
    "     \"luvv\": \"love\",\n",
    "    \"xoxo\": \"kiss\",\n",
    "    \"bc\" : \"because\",\n",
    "    \"bcuz\": \"because\",\n",
    "    \"cuze\": \"because\",\n",
    "    \"cuz\": \"because\",\n",
    "    \"lil\": \"little\",\n",
    "    \"fam\": \"family\",\n",
    "    \"bro\": \"brother\",\n",
    "    \"sis\": \"sister\",\n",
    "    \"thang\": \"thing\",\n",
    "    \"aint\": \"is not\",\n",
    "    \"tryna\": \"try to\",\n",
    "    \"neva\": \"never\",\n",
    "    \"bday\": \"birthday\",\n",
    "    \"gr8\": \"great\",\n",
    "    \"4ever\": \"forever\",\n",
    "    \"nvm\": \"never mind\",\n",
    "    \"r\" : \"are\",\n",
    "    \"tryin\": \"trying\",\n",
    "    \"2morow\" : \"tomorrow\",\n",
    "    \"2moro\" : \"tomorrow\",\n",
    "    \"morow\" : \"tomorrow\",\n",
    "    \"tmrw\" : \"tomorrow\",\n",
    "    \"tmrow\" : \"tomorrow\",\n",
    "    \"2morow\" : \"tomorrow\",\n",
    "    \"2morro\" : \"tomorrow\",\n",
    "    \"morrow\" : \"tomorrow\",\n",
    "    \"tmrrw\" : \"tomorrow\",\n",
    "    \"tmrrow\" : \"tomorrow\",\n",
    "    \"b4\" : \"before\",\n",
    "    \"every1\" : \"everyone\",\n",
    "    \"2nd\" : \"second\",\n",
    "     \"h8\" : \"hate\",\n",
    "    \"ppl\" : \"people\",\n",
    "    \"ly\" : \"love you\",\n",
    "    \"2nite\" : \"tonight\",\n",
    "    \"2night\" : \"tonight\",\n",
    "    \"tonite\" : \"tonight\",\n",
    "    \"bday\" : \"birthday\",\n",
    "    \"2day\" : \"today\",\n",
    "   \"1st\" : \"first\",\n",
    "    \"3rd\" : \"third\",\n",
    "    \"str8\" : \"straight\",\n",
    "    \"fk\" : \"fuck\",\n",
    "    \"fkin\" : \"fucking\",\n",
    "    \"fck\" : \"fuck\",\n",
    "    \"fcking\": \"fucking\",\n",
    "    \"fuckin\": \"fucking\",\n",
    "    \"wit\": \"with\",\n",
    "    \"fri\":\"friday\",\n",
    "    \"friggin\": \"fucking\",\n",
    "    \"frigging\": \"fucking\",\n",
    "    \"lovin\": \"loving\",\n",
    "    \"luving\": \"loving\",\n",
    "   \"missin\": \"missing\",\n",
    "   \"freakin\":\"freaking\",\n",
    "   \"killin\":\"killing\",\n",
    "    \"wat\":\"what\",\n",
    "   \"em\":\"them\",\n",
    "   \"hatin\" : \"hating\",\n",
    "    \"recieve\": \"receive\",\n",
    "    \"seperated\": \"separated\",\n",
    "    \"wierd\": \"weird\",\n",
    "    \"loosing\": \"losing\",\n",
    "    \"thier\": \"their\",\n",
    "    \"thx\": \"thanks\",\n",
    "    \"ty\": \"thank you\",\n",
    "    \"pls\": \"please\",\n",
    "    \"plz\": \"please\",\n",
    "    \"skool\":\"school\",\n",
    "    \"frnd\":\"friend\",\n",
    "    \"frnds\":\"friends\",\n",
    "    \"belive\":\"believe\",\n",
    "    \"seein\":\"seeing\",\n",
    "    \"kno\":\"know\",\n",
    "    \"icant\":\"i cant\",\n",
    "    \"bein\":\"being\",\n",
    "    \"bout\":\"about\",\n",
    "    \"wen\":\"when\",\n",
    "    \"jst\":\"just\",\n",
    "    \"xx\":\"kiss\"\n",
    "}\n",
    "def replace_slang(text):\n",
    "    words = text.split()\n",
    "    replaced_words = [slang_dict.get(word, word) for word in words]\n",
    "    return \" \".join(replaced_words)\n",
    "\n",
    "def remove_short_words(text):\n",
    "    return ' '.join(word for word in text.split() if len(word) > 1)\n",
    "\n",
    "def remove_extra_spaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def replace_urls(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', ' url ', text)\n",
    "\n",
    "def reduce_repeated_letters_to_two(word):\n",
    "    return re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", word)\n",
    "\n",
    "def expand_contractions(text):\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions_dict.keys()) + r')\\b')\n",
    "    return pattern.sub(lambda x: contractions_dict[x.group()], text)\n",
    "\n",
    "def handle_negations(text):\n",
    "    negation_words = {'not', 'never', 'no'}\n",
    "    skip_words = {'a', 'an'}  # words to skip after negation\n",
    "    tokens = text.split()\n",
    "    result = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(tokens):\n",
    "        token = tokens[i]\n",
    "        result.append(token)\n",
    "        if token.lower() in negation_words:\n",
    "            j = i + 1\n",
    "            # Skip \"a\", \"an\", etc.\n",
    "            while j < len(tokens) and tokens[j].lower() in skip_words:\n",
    "                result.append(tokens[j])\n",
    "                j += 1\n",
    "            if j < len(tokens):\n",
    "                result.append('NOT_' + tokens[j])\n",
    "                i = j  \n",
    "        i += 1\n",
    "\n",
    "    return ' '.join(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d4e9b1",
   "metadata": {
    "papermill": {
     "duration": 0.007192,
     "end_time": "2025-05-04T13:59:45.618175",
     "exception": false,
     "start_time": "2025-05-04T13:59:45.610983",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Cleaning Text Function Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5128e8e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T13:59:45.633663Z",
     "iopub.status.busy": "2025-05-04T13:59:45.633461Z",
     "iopub.status.idle": "2025-05-04T13:59:45.642612Z",
     "shell.execute_reply": "2025-05-04T13:59:45.642128Z"
    },
    "papermill": {
     "duration": 0.018162,
     "end_time": "2025-05-04T13:59:45.643580",
     "exception": false,
     "start_time": "2025-05-04T13:59:45.625418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. NO PREPROCESSING\n",
    "def clean_text_1(text):\n",
    "    return text\n",
    "\n",
    "# 2. ONLY FIX_MOJ AND LOWER\n",
    "def clean_text_2(text):\n",
    "    text = fix_mojibake(text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# 3. ONLY FIX_MOJ AND LOWER + URL + MENTION\n",
    "def clean_text_3(text):\n",
    "    text = fix_mojibake(text)\n",
    "    text = text.lower()\n",
    "    text = replace_urls(text)\n",
    "    text = replace_mentions(text)\n",
    "    return text\n",
    "\n",
    "# 4. ONLY FIX_MOJ AND LOWER + URL + MENTION + REPETITION TO 2\n",
    "def clean_text_4(text):\n",
    "    text = fix_mojibake(text)\n",
    "    text = text.lower()\n",
    "    text = replace_urls(text)\n",
    "    text = replace_mentions(text)\n",
    "    text = reduce_repeated_letters_to_two(text)\n",
    "    return text\n",
    "\n",
    "# 5. + EMOTICON + PUNCT REMOV/REPLACE + EXTRA SPACES\n",
    "def clean_text_5(text):\n",
    "    text = fix_mojibake(text)\n",
    "    text = text.lower()\n",
    "    text = replace_urls(text)\n",
    "    text = replace_mentions(text)\n",
    "    text = reduce_repeated_letters_to_two(text)\n",
    "    text = replace_emoticon(text)\n",
    "    text = remove_specific_punctuation(text)\n",
    "    text = replace_punctuation_with_space(text)\n",
    "    text = remove_extra_spaces(text)\n",
    "    return text\n",
    "\n",
    "# 6. + SLANG \n",
    "def clean_text_6(text):\n",
    "    text = fix_mojibake(text)\n",
    "    text = text.lower()\n",
    "    text = replace_urls(text)\n",
    "    text = replace_mentions(text)\n",
    "    text = reduce_repeated_letters_to_two(text)\n",
    "    text = replace_emoticon(text)\n",
    "    text = remove_specific_punctuation(text)\n",
    "    text = replace_punctuation_with_space(text)\n",
    "    text = replace_slang(text)  \n",
    "    text = remove_extra_spaces(text)\n",
    "    return text\n",
    "\n",
    "# 7. + SHORT WORD REMOVAL\n",
    "def clean_text_7(text):\n",
    "    text = fix_mojibake(text)\n",
    "    text = text.lower()\n",
    "    text = replace_urls(text)\n",
    "    text = replace_mentions(text)\n",
    "    text = reduce_repeated_letters_to_two(text)\n",
    "    text = replace_emoticon(text)\n",
    "    text = remove_specific_punctuation(text)\n",
    "    text = replace_punctuation_with_space(text)\n",
    "    text = replace_slang(text) \n",
    "    text = remove_short_words(text)\n",
    "    text = remove_extra_spaces(text)\n",
    "    return text\n",
    "\n",
    "# 8. + CONTRACTIONS + NEGATION\n",
    "def clean_text(text):\n",
    "    text = fix_mojibake(text)\n",
    "    text = text.lower()\n",
    "    text = replace_urls(text)\n",
    "    text = replace_mentions(text)\n",
    "    text = reduce_repeated_letters_to_two(text)\n",
    "    text = replace_emoticon(text)\n",
    "    text = remove_specific_punctuation(text)\n",
    "    text = replace_punctuation_with_space(text)\n",
    "    text = replace_slang(text)  \n",
    "    text = remove_short_words(text)\n",
    "    text = expand_contractions(text)\n",
    "    text = handle_negations(text)\n",
    "    text = remove_extra_spaces(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "clean_text_versions = {\n",
    "    \"no_preprocessing\": clean_text_1,\n",
    "    \"fixmoj_lower\": clean_text_2,\n",
    "    \"fixmoj_lower_url_mention\": clean_text_3,\n",
    "    \"fixmoj_lower_url_mention_repetition\": clean_text_4,\n",
    "    \"fixmoj_lower_url_mention_repetition_emotic_punct\": clean_text_5,\n",
    "    \"fixmoj_lower_url_mention_repetition_emotic_punct_slang\": clean_text_6,\n",
    "    \"fixmoj_lower_url_mention_repetition_emotic_punct_slang_short\": clean_text_7,\n",
    "    \"fixmoj_lower_url_mention_repetition_emotic_punct_slang_short_contraction_negation\": clean_text\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# for name, clean_text_func in clean_text_versions.items():\n",
    "#     print(f\"\\n=== Running with {name} ===\\n\")\n",
    "    \n",
    "#     train_df = pd.read_csv(\"/kaggle/input/ai-2-dl-for-nlp-2025-homework-2/train_dataset.csv\")\n",
    "#     val_df = pd.read_csv(\"/kaggle/input/ai-2-dl-for-nlp-2025-homework-2/val_dataset.csv\")\n",
    "\n",
    "#     train_df[\"Text\"] = train_df[\"Text\"].apply(clean_text_func)\n",
    "#     val_df[\"Text\"] = val_df[\"Text\"].apply(clean_text_func)\n",
    "\n",
    "#     train_tokens = train_df[\"Text\"].astype(str).apply(lambda x: x.split())\n",
    "#     val_tokens   = val_df[\"Text\"].astype(str).apply(lambda x: x.split())\n",
    "#     w2v_model = Word2Vec(sentences=train_tokens, vector_size=EMBEDDING_DIM, sg=1, min_count=1, seed=SEED, workers=1)\n",
    "\n",
    "#     train_df[\"vector\"] = train_tokens.apply(lambda tokens: get_tweet_vector(tokens, w2v_model, EMBEDDING_DIM))\n",
    "#     val_df[\"vector\"]   = val_tokens.apply(lambda tokens: get_tweet_vector(tokens, w2v_model, EMBEDDING_DIM))\n",
    "\n",
    "#     X_train = np.vstack(train_df[\"vector\"].values)\n",
    "#     y_train = train_df[\"Label\"].values\n",
    "#     X_val = np.vstack(val_df[\"vector\"].values)\n",
    "#     y_val = val_df[\"Label\"].values\n",
    "\n",
    "#     x_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "#     y_train_tensor = torch.tensor(y_train, dtype=torch.float32) \n",
    "#     x_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "#     y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "#     train_dataset = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "#     val_dataset = torch.utils.data.TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "#     g = torch.Generator()\n",
    "#     g.manual_seed(SEED)\n",
    "#     train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, generator=g)\n",
    "#     val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "#     torch.manual_seed(SEED)\n",
    "#     model = BaselineNet().to(DEVICE)\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "#     optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "#     for epoch in range(5):\n",
    "#         model.train()\n",
    "#         batch_losses = []\n",
    "\n",
    "#         for x_batch, y_batch in train_loader:\n",
    "#             x_batch = x_batch.to(DEVICE)\n",
    "#             y_batch = y_batch.to(DEVICE).unsqueeze(1)\n",
    "\n",
    "#             y_pred = model(x_batch)\n",
    "#             loss = criterion(y_pred, y_batch)\n",
    "#             batch_losses.append(loss.item())\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#     model.eval()\n",
    "#     val_preds, val_labels = [], []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for x_batch, y_batch in val_loader:\n",
    "#             x_batch = x_batch.to(DEVICE)\n",
    "#             outputs = model(x_batch)\n",
    "#             preds = (torch.sigmoid(outputs).cpu().numpy() > 0.5).astype(int)\n",
    "#             val_preds.extend(preds.flatten())\n",
    "#             val_labels.extend(y_batch.cpu().numpy().astype(int))\n",
    "\n",
    "#     acc = accuracy_score(val_labels, val_preds)\n",
    "#     prec, rec, f1, _ = precision_recall_fscore_support(val_labels, val_preds, average='weighted')\n",
    "#     print(f\"Validation → Acc: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35df5cd0",
   "metadata": {
    "papermill": {
     "duration": 0.007039,
     "end_time": "2025-05-04T13:59:45.657733",
     "exception": false,
     "start_time": "2025-05-04T13:59:45.650694",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# From Text to Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "854ce9af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T13:59:45.673795Z",
     "iopub.status.busy": "2025-05-04T13:59:45.673583Z",
     "iopub.status.idle": "2025-05-04T14:01:22.263250Z",
     "shell.execute_reply": "2025-05-04T14:01:22.262646Z"
    },
    "papermill": {
     "duration": 96.59926,
     "end_time": "2025-05-04T14:01:22.264724",
     "exception": false,
     "start_time": "2025-05-04T13:59:45.665464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/kaggle/input/ai-2-dl-for-nlp-2025-homework-2/train_dataset.csv\")\n",
    "val_df   = pd.read_csv(\"/kaggle/input/ai-2-dl-for-nlp-2025-homework-2/val_dataset.csv\")\n",
    "test_df = pd.read_csv(\"/kaggle/input/ai-2-dl-for-nlp-2025-homework-2/test_dataset.csv\")\n",
    "\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df[\"Text\"] = df[\"Text\"].apply(clean_text)\n",
    "\n",
    "train_tokens = train_df[\"Text\"].astype(str).apply(lambda x: x.split())\n",
    "val_tokens   = val_df[\"Text\"].astype(str).apply(lambda x: x.split())\n",
    "test_tokens = test_df[\"Text\"].astype(str).apply(lambda x: x.split())\n",
    "w2v_model = Word2Vec(sentences=train_tokens, vector_size=EMBEDDING_DIM, sg=1, min_count=1, workers=1, seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a9f30d",
   "metadata": {
    "papermill": {
     "duration": 0.00728,
     "end_time": "2025-05-04T14:01:22.323705",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.316425",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7e90bb",
   "metadata": {
    "papermill": {
     "duration": 0.007005,
     "end_time": "2025-05-04T14:01:22.337861",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.330856",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Aggregation function Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec21c967",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:01:22.353764Z",
     "iopub.status.busy": "2025-05-04T14:01:22.353467Z",
     "iopub.status.idle": "2025-05-04T14:01:22.368956Z",
     "shell.execute_reply": "2025-05-04T14:01:22.368405Z"
    },
    "papermill": {
     "duration": 0.025051,
     "end_time": "2025-05-04T14:01:22.370008",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.344957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_aggregations_binary(aggregations=[\"mean\", \"sum\", \"max\"]):\n",
    "\n",
    "    for agg in aggregations:\n",
    "        train_df[\"vector\"] = train_tokens.apply(lambda tokens: get_tweet_vector(tokens, w2v_model, EMBEDDING_DIM, agg=agg))\n",
    "        val_df[\"vector\"]   = val_tokens.apply(lambda tokens: get_tweet_vector(tokens, w2v_model, EMBEDDING_DIM, agg=agg))\n",
    "\n",
    "        X_train = np.vstack(train_df[\"vector\"].values)\n",
    "        y_train = train_df[\"Label\"].values\n",
    "        X_val   = np.vstack(val_df[\"vector\"].values)\n",
    "        y_val   = val_df[\"Label\"].values\n",
    "\n",
    "        x_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "        x_val_tensor   = torch.tensor(X_val, dtype=torch.float32)\n",
    "        y_val_tensor   = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        train_dataset = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "        val_dataset   = torch.utils.data.TensorDataset(x_val_tensor, y_val_tensor)\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(SEED)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, generator=g)\n",
    "        val_loader   = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "        torch.manual_seed(SEED)\n",
    "        model = BaselineNet().to(DEVICE)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "        for epoch in range(5):\n",
    "            model.train()\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                y_pred = model(xb)\n",
    "                loss = criterion(y_pred, yb)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_preds, val_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                probs = torch.sigmoid(model(xb))\n",
    "                preds = (probs > 0.5).int().cpu().numpy()\n",
    "                val_preds.extend(preds.flatten())\n",
    "                val_labels.extend(yb.cpu().numpy().flatten().astype(int))\n",
    "\n",
    "        acc = accuracy_score(val_labels, val_preds)\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(val_labels, val_preds, average='weighted')\n",
    "\n",
    "        print(f\"Results with {agg.upper()} pooling:\")\n",
    "        print(f\"  Accuracy : {acc:.4f}\")\n",
    "        print(f\"  Precision: {prec:.4f}\")\n",
    "        print(f\"  Recall   : {rec:.4f}\")\n",
    "        print(f\"  F1 Score : {f1:.4f}\")\n",
    "        \n",
    "#evaluate_aggregations_binary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c5833f",
   "metadata": {
    "papermill": {
     "duration": 0.007227,
     "end_time": "2025-05-04T14:01:22.384449",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.377222",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Features Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c94475c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:01:22.400334Z",
     "iopub.status.busy": "2025-05-04T14:01:22.400148Z",
     "iopub.status.idle": "2025-05-04T14:01:22.408740Z",
     "shell.execute_reply": "2025-05-04T14:01:22.408184Z"
    },
    "papermill": {
     "duration": 0.017899,
     "end_time": "2025-05-04T14:01:22.409754",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.391855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_scaler_settings():\n",
    "    results = []\n",
    "    train_vectors = np.vstack([get_tweet_vector(tokens, w2v_model, EMBEDDING_DIM, agg=\"sum\") for tokens in train_tokens])\n",
    "    val_vectors   = np.vstack([get_tweet_vector(tokens, w2v_model, EMBEDDING_DIM, agg=\"sum\") for tokens in val_tokens])\n",
    "\n",
    "    for with_mean in [True, False]:\n",
    "        for with_std in [True, False]:\n",
    "            print(f\"\\n Testing with_mean={with_mean}, with_std={with_std}\")\n",
    "            \n",
    "            scaler = StandardScaler(with_mean=with_mean, with_std=with_std)\n",
    "            train_scaled = scaler.fit_transform(train_vectors)\n",
    "            val_scaled   = scaler.transform(val_vectors)\n",
    "\n",
    "            x_train_tensor = torch.tensor(train_scaled, dtype=torch.float32)\n",
    "            y_train_tensor = torch.tensor(train_df[\"Label\"].values, dtype=torch.float32)\n",
    "            x_val_tensor   = torch.tensor(val_scaled, dtype=torch.float32)\n",
    "            y_val_tensor   = torch.tensor(val_df[\"Label\"].values, dtype=torch.float32)\n",
    "\n",
    "            train_dataset = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "            val_dataset   = torch.utils.data.TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "            g = torch.Generator()\n",
    "            g.manual_seed(SEED)\n",
    "            train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, generator=g)\n",
    "            val_loader   = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "            torch.manual_seed(SEED)\n",
    "            model = BaselineNet().to(DEVICE)\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "            for epoch in range(5):\n",
    "                model.train()\n",
    "                for xb, yb in train_loader:\n",
    "                    xb = xb.to(DEVICE)\n",
    "                    yb = yb.to(DEVICE).unsqueeze(1)\n",
    "                    pred = model(xb)\n",
    "                    loss = criterion(pred, yb)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            val_preds, val_labels = [], []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    xb = xb.to(DEVICE)\n",
    "                    probs = torch.sigmoid(model(xb))\n",
    "                    preds = (probs > 0.5).int().cpu().numpy()\n",
    "                    val_preds.extend(preds.flatten())\n",
    "                    val_labels.extend(yb.cpu().numpy().flatten().astype(int))\n",
    "\n",
    "            acc = accuracy_score(val_labels, val_preds)\n",
    "            prec, rec, f1, _ = precision_recall_fscore_support(val_labels, val_preds, average='weighted')\n",
    "\n",
    "            print(f\"   Acc: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}\")\n",
    "            results.append(((with_mean, with_std), acc, prec, rec, f1))\n",
    "\n",
    "    return results\n",
    "    \n",
    "# evaluate_scaler_settings()\n",
    "# for (mean, std), acc, prec, rec, f1 in results:\n",
    "#     print(f\"with_mean={mean}, with_std={std} → F1: {f1:.4f} | Acc: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e3c5",
   "metadata": {
    "papermill": {
     "duration": 0.007156,
     "end_time": "2025-05-04T14:01:22.424649",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.417493",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2 Layers Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14ab86e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:01:22.440003Z",
     "iopub.status.busy": "2025-05-04T14:01:22.439809Z",
     "iopub.status.idle": "2025-05-04T14:01:22.444817Z",
     "shell.execute_reply": "2025-05-04T14:01:22.444300Z"
    },
    "papermill": {
     "duration": 0.013926,
     "end_time": "2025-05-04T14:01:22.445799",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.431873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, dropout=0.0, activation=\"relu\"):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in, H1)\n",
    "        self.linear2 = nn.Linear(H1, H2)\n",
    "        self.linear3 = nn.Linear(H2, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activate(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activate(self.linear2(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.linear3(x)\n",
    "\n",
    "    def activate(self, x):\n",
    "        if self.activation == \"relu\":\n",
    "            return F.relu(x)\n",
    "        elif self.activation == \"leaky_relu\":\n",
    "            return F.leaky_relu(x)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {self.activation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f84f95",
   "metadata": {
    "papermill": {
     "duration": 0.007297,
     "end_time": "2025-05-04T14:01:22.460334",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.453037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Parameters Turing - Broad Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "841ad540",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:01:22.475980Z",
     "iopub.status.busy": "2025-05-04T14:01:22.475788Z",
     "iopub.status.idle": "2025-05-04T14:01:22.485907Z",
     "shell.execute_reply": "2025-05-04T14:01:22.485374Z"
    },
    "papermill": {
     "duration": 0.019235,
     "end_time": "2025-05-04T14:01:22.486946",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.467711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    set_seed(42)\n",
    "\n",
    "    vector_size = trial.suggest_categorical(\"vector_size\", [200, 300])\n",
    "    window = trial.suggest_int(\"window\", 3, 7)\n",
    "    min_count = trial.suggest_int(\"min_count\", 1, 5)\n",
    "    sg = 1\n",
    "\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences=train_tokens,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        sg=sg,\n",
    "        workers=1,\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    X_train = np.vstack(train_tokens.apply(lambda tokens: get_tweet_vector(tokens, w2v_model, embedding_dim=vector_size, agg=\"sum\")).values)\n",
    "    X_val   = np.vstack(val_tokens.apply(lambda tokens: get_tweet_vector(tokens, w2v_model, embedding_dim=vector_size, agg=\"sum\")).values)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val   = scaler.transform(X_val)\n",
    "\n",
    "    y_train = train_df[\"Label\"].values\n",
    "    y_val   = val_df[\"Label\"].values\n",
    "\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.long)\n",
    "    )\n",
    "    val_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "        torch.tensor(y_val, dtype=torch.long)\n",
    "    )\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=g)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    H1 = trial.suggest_int(\"H1\", 128, 512)\n",
    "    H2 = trial.suggest_int(\"H2\", 64, H1)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"leaky_relu\"])\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adam\", \"adamw\"])\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    torch.manual_seed(SEED)\n",
    "    model = TwoLayerNet(vector_size, H1, H2, dropout=dropout, activation=activation).to(DEVICE)\n",
    "\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE).unsqueeze(1).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            outputs = model(xb)\n",
    "            preds = (torch.sigmoid(outputs).cpu().numpy() > 0.5).astype(int)\n",
    "            val_preds.extend(preds.flatten())\n",
    "            val_labels.extend(yb.numpy())\n",
    "\n",
    "    acc = accuracy_score(val_labels, val_preds)\n",
    "    trial.report(acc, step=0)\n",
    "\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return acc\n",
    "    \n",
    "# pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=2)\n",
    "# study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "# study.optimize(objective, n_trials=40, n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b318daf",
   "metadata": {
    "papermill": {
     "duration": 0.007176,
     "end_time": "2025-05-04T14:01:22.501316",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.494140",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Parameters Turing - Refined Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "847347ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:01:22.516737Z",
     "iopub.status.busy": "2025-05-04T14:01:22.516564Z",
     "iopub.status.idle": "2025-05-04T14:01:22.525956Z",
     "shell.execute_reply": "2025-05-04T14:01:22.525421Z"
    },
    "papermill": {
     "duration": 0.01844,
     "end_time": "2025-05-04T14:01:22.526974",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.508534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    set_seed(42)\n",
    "    vector_size = 300\n",
    "    window = trial.suggest_int(\"window\", 6, 7)\n",
    "    min_count = trial.suggest_int(\"min_count\", 3, 4)\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences=train_tokens,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        sg=1,\n",
    "        workers=1,\n",
    "        seed=SEED\n",
    "    )\n",
    "\n",
    "    X_train = np.vstack([get_tweet_vector(tokens, w2v_model, vector_size, agg=\"sum\") for tokens in train_tokens])\n",
    "    X_val = np.vstack([get_tweet_vector(tokens, w2v_model, vector_size, agg=\"sum\") for tokens in val_tokens])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    y_train = train_df[\"Label\"].values.astype(np.float32)\n",
    "    y_val = val_df[\"Label\"].values.astype(np.float32)\n",
    "\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128])\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train).unsqueeze(1))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val).unsqueeze(1))\n",
    "\n",
    "    g = torch.Generator().manual_seed(SEED)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=g)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    H1 = trial.suggest_int(\"H1\", 350, 480)\n",
    "    H2 = trial.suggest_int(\"H2\", 200, H1)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.25, 0.3)\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"leaky_relu\"])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 3e-4, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-4, log=True)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adam\", \"adamw\"])\n",
    "    \n",
    "    torch.manual_seed(SEED)\n",
    "    model = TwoLayerNet(\n",
    "        D_in=vector_size, H1=H1, H2=H2,\n",
    "        dropout=dropout, activation=activation\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    optimizer = (\n",
    "        torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        if optimizer_name == \"adam\"\n",
    "        else torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    )\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    model.train()\n",
    "    for _ in range(5):\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            preds = torch.sigmoid(model(xb)).cpu().numpy() > 0.5\n",
    "            val_preds.extend(preds.flatten())\n",
    "            val_labels.extend(yb.cpu().numpy().flatten())\n",
    "\n",
    "    acc = accuracy_score(val_labels, val_preds)\n",
    "    trial.report(acc, step=0)\n",
    "    \n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return acc\n",
    "        \n",
    "# pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=2)\n",
    "# study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "# study.optimize(objective, n_trials=40, n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d3d570",
   "metadata": {
    "papermill": {
     "duration": 0.00717,
     "end_time": "2025-05-04T14:01:22.541379",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.534209",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3 Layers Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "461af834",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:01:22.556879Z",
     "iopub.status.busy": "2025-05-04T14:01:22.556686Z",
     "iopub.status.idle": "2025-05-04T14:01:22.561951Z",
     "shell.execute_reply": "2025-05-04T14:01:22.561422Z"
    },
    "papermill": {
     "duration": 0.014292,
     "end_time": "2025-05-04T14:01:22.562927",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.548635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ThreeLayerNet(nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, H3, dropout=0.0, activation='relu'):\n",
    "        super(ThreeLayerNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in, H1)\n",
    "        self.linear2 = nn.Linear(H1, H2)\n",
    "        self.linear3 = nn.Linear(H2, H3)\n",
    "        self.linear4 = nn.Linear(H3, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activate(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activate(self.linear2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activate(self.linear3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear4(x) \n",
    "        return x\n",
    "\n",
    "    def activate(self, x):\n",
    "        if self.activation == 'relu':\n",
    "            return F.relu(x)\n",
    "        elif self.activation == 'leaky_relu':\n",
    "            return F.leaky_relu(x)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {self.activation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28bc3f5",
   "metadata": {
    "papermill": {
     "duration": 0.007234,
     "end_time": "2025-05-04T14:01:22.577367",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.570133",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Parameters Turing - Broad Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3964a85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:01:22.593314Z",
     "iopub.status.busy": "2025-05-04T14:01:22.593117Z",
     "iopub.status.idle": "2025-05-04T14:01:22.603218Z",
     "shell.execute_reply": "2025-05-04T14:01:22.602698Z"
    },
    "papermill": {
     "duration": 0.0193,
     "end_time": "2025-05-04T14:01:22.604166",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.584866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    set_seed(42)\n",
    "    vector_size = trial.suggest_categorical(\"vector_size\", [200, 300])\n",
    "    window = trial.suggest_int(\"window\", 4, 8)\n",
    "    min_count = trial.suggest_int(\"min_count\", 1, 5)\n",
    "\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences=train_tokens,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        sg=1,\n",
    "        workers=1,\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    EMBEDDING_DIM = vector_size\n",
    "\n",
    "    X_train = np.vstack([\n",
    "        get_tweet_vector(tokens, w2v_model, embedding_dim=vector_size, agg=\"sum\")\n",
    "        for tokens in train_tokens\n",
    "    ])\n",
    "    X_val = np.vstack([\n",
    "        get_tweet_vector(tokens, w2v_model, embedding_dim=vector_size, agg=\"sum\")\n",
    "        for tokens in val_tokens\n",
    "    ])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    y_train = train_df[\"Label\"].values\n",
    "    y_val = val_df[\"Label\"].values\n",
    "\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.long)\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "        torch.tensor(y_val, dtype=torch.long)\n",
    "    )\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    H1 = trial.suggest_int(\"H1\", 128, 512)\n",
    "    H2 = trial.suggest_int(\"H2\", 64, H1)\n",
    "    H3 = trial.suggest_int(\"H3\", 32, H2)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"leaky_relu\"])\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adam\", \"adamw\"])\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    \n",
    "    torch.manual_seed(SEED)\n",
    "    model = ThreeLayerNet(\n",
    "        D_in=EMBEDDING_DIM,\n",
    "        H1=H1,\n",
    "        H2=H2,\n",
    "        H3=H3,\n",
    "        dropout=dropout,\n",
    "        activation=activation\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE).unsqueeze(1).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            preds = model(xb)\n",
    "            preds = (torch.sigmoid(preds) > 0.5).long().squeeze(1).cpu().numpy()\n",
    "            val_preds.extend(preds)\n",
    "            val_labels.extend(yb.numpy())\n",
    "\n",
    "    acc = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "    trial.report(acc, step=0)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "# pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=2)\n",
    "# study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "# study.optimize(objective, n_trials=40, n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ecc1ab",
   "metadata": {
    "papermill": {
     "duration": 0.00722,
     "end_time": "2025-05-04T14:01:22.618960",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.611740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Parameters Turing - Refined Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9cee2db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:01:22.634130Z",
     "iopub.status.busy": "2025-05-04T14:01:22.633961Z",
     "iopub.status.idle": "2025-05-04T14:01:22.643536Z",
     "shell.execute_reply": "2025-05-04T14:01:22.642989Z"
    },
    "papermill": {
     "duration": 0.018382,
     "end_time": "2025-05-04T14:01:22.644565",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.626183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    set_seed(42)\n",
    "    vector_size = 300  \n",
    "    window = trial.suggest_int(\"window\", 6, 7)\n",
    "    min_count = trial.suggest_int(\"min_count\", 3, 4)\n",
    "\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences=train_tokens,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        sg=1,\n",
    "        workers=1,\n",
    "        seed=SEED,\n",
    "        epochs=5\n",
    "    )\n",
    "\n",
    "    X_train = np.vstack([\n",
    "        get_tweet_vector(tokens, w2v_model, vector_size, agg=\"sum\")\n",
    "        for tokens in train_tokens\n",
    "    ])\n",
    "    X_val = np.vstack([\n",
    "        get_tweet_vector(tokens, w2v_model, vector_size, agg=\"sum\")\n",
    "        for tokens in val_tokens\n",
    "    ])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    y_train = train_df[\"Label\"].values.astype(np.float32)\n",
    "    y_val = val_df[\"Label\"].values.astype(np.float32)\n",
    "\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128])\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "        torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "    )\n",
    "    g = torch.Generator().manual_seed(SEED)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=g)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    H1 = trial.suggest_int(\"H1\", 440, 480)\n",
    "    H2 = trial.suggest_int(\"H2\", 360, H1)\n",
    "    H3 = trial.suggest_int(\"H3\", 290, H2)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.13, 0.21)\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"relu\"])\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adamw\"])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 3e-4, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-4, log=True)\n",
    "\n",
    "    model = ThreeLayerNet(\n",
    "        D_in=vector_size,\n",
    "        H1=H1, H2=H2, H3=H3,\n",
    "        dropout=dropout,\n",
    "        activation=activation\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    if optimizer_name == \"adamw\":\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError(\"Only AdamW supported in refined search\")\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            preds = (torch.sigmoid(model(xb)) > 0.5).float().cpu().numpy()\n",
    "            val_preds.extend(preds)\n",
    "            val_labels.extend(yb.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "    trial.report(acc, step=0)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return acc\n",
    "\n",
    "# pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=2)\n",
    "# study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "# study.optimize(objective, n_trials=40, n_jobs=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17025d2",
   "metadata": {
    "papermill": {
     "duration": 0.007491,
     "end_time": "2025-05-04T14:01:22.659210",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.651719",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4 Layers Model - Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27e54631",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:01:22.674372Z",
     "iopub.status.busy": "2025-05-04T14:01:22.674204Z",
     "iopub.status.idle": "2025-05-04T14:01:22.679437Z",
     "shell.execute_reply": "2025-05-04T14:01:22.678958Z"
    },
    "papermill": {
     "duration": 0.013986,
     "end_time": "2025-05-04T14:01:22.680534",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.666548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FourLayerNet(nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, H3, H4, D_out=1, dropout=0.0, activation='relu'):\n",
    "        super(FourLayerNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in, H1)\n",
    "        self.linear2 = nn.Linear(H1, H2)\n",
    "        self.linear3 = nn.Linear(H2, H3)\n",
    "        self.linear4 = nn.Linear(H3, H4)\n",
    "        self.linear5 = nn.Linear(H4, D_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = activation\n",
    "\n",
    "    def activate(self, x):\n",
    "        if self.activation == 'relu':\n",
    "            return F.relu(x)\n",
    "        elif self.activation == 'leaky_relu':\n",
    "            return F.leaky_relu(x)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {self.activation}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.activate(self.linear1(x)))\n",
    "        x = self.dropout(self.activate(self.linear2(x)))\n",
    "        x = self.dropout(self.activate(self.linear3(x)))\n",
    "        x = self.dropout(self.activate(self.linear4(x)))\n",
    "        return self.linear5(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412cc6cf",
   "metadata": {
    "papermill": {
     "duration": 0.007123,
     "end_time": "2025-05-04T14:01:22.694877",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.687754",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Parameters Turing - Broad Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5697d394",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:01:22.710059Z",
     "iopub.status.busy": "2025-05-04T14:01:22.709895Z",
     "iopub.status.idle": "2025-05-04T14:01:22.720606Z",
     "shell.execute_reply": "2025-05-04T14:01:22.719927Z"
    },
    "papermill": {
     "duration": 0.019588,
     "end_time": "2025-05-04T14:01:22.721659",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.702071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    set_seed(42)\n",
    "    vector_size = trial.suggest_categorical(\"vector_size\", [200, 300])\n",
    "    window = trial.suggest_int(\"window\", 4, 8)\n",
    "    min_count = trial.suggest_int(\"min_count\", 1, 5)\n",
    "\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences=train_tokens,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        sg=1,\n",
    "        workers=1,\n",
    "        epochs=5,\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    EMBEDDING_DIM = vector_size\n",
    "\n",
    "    X_train = np.vstack([\n",
    "        get_tweet_vector(tokens, w2v_model, embedding_dim=vector_size, agg=\"sum\", seed=SEED)\n",
    "        for tokens in train_tokens\n",
    "    ])\n",
    "    X_val = np.vstack([\n",
    "        get_tweet_vector(tokens, w2v_model, embedding_dim=vector_size, agg=\"sum\", seed=SEED)\n",
    "        for tokens in val_tokens\n",
    "    ])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    y_train = train_df[\"Label\"].values.astype(np.float32)\n",
    "    y_val = val_df[\"Label\"].values.astype(np.float32)\n",
    "\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "        torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "    )\n",
    "\n",
    "    g = torch.Generator().manual_seed(SEED)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=g)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    H1 = trial.suggest_int(\"H1\", 256, 512)\n",
    "    H2 = trial.suggest_int(\"H2\", 128, H1)\n",
    "    H3 = trial.suggest_int(\"H3\", 64, H2)\n",
    "    H4 = trial.suggest_int(\"H4\", 32, H3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"leaky_relu\"])\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adam\", \"adamw\"])\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    model = FourLayerNet(\n",
    "        D_in=EMBEDDING_DIM,\n",
    "        H1=H1, H2=H2, H3=H3, H4=H4,\n",
    "        dropout=dropout,\n",
    "        activation=activation\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for _ in range(5):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            preds = torch.sigmoid(model(xb)) > 0.5\n",
    "            val_preds.extend(preds.cpu().squeeze().int().numpy())\n",
    "            val_labels.extend(yb.cpu().squeeze().int().numpy())\n",
    "\n",
    "    acc = accuracy_score(val_labels, val_preds)\n",
    "    trial.report(acc, step=0)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    return acc\n",
    "\n",
    "\n",
    "# pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=2)\n",
    "# study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "# study.optimize(objective, n_trials=40, n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d87f4fd",
   "metadata": {
    "papermill": {
     "duration": 0.007141,
     "end_time": "2025-05-04T14:01:22.736106",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.728965",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Parameters Turing - Refined Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6acf40e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:01:22.751646Z",
     "iopub.status.busy": "2025-05-04T14:01:22.751266Z",
     "iopub.status.idle": "2025-05-04T14:01:22.761714Z",
     "shell.execute_reply": "2025-05-04T14:01:22.761220Z"
    },
    "papermill": {
     "duration": 0.019424,
     "end_time": "2025-05-04T14:01:22.762778",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.743354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    set_seed(42)\n",
    "    vector_size = 300\n",
    "    window = trial.suggest_int(\"window\", 6, 8)\n",
    "    min_count = trial.suggest_int(\"min_count\", 2, 5)\n",
    "\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences=train_tokens,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        sg=1,\n",
    "        workers=1,\n",
    "        epochs=5,\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    EMBEDDING_DIM = vector_size\n",
    "\n",
    "    X_train = np.vstack([\n",
    "        get_tweet_vector(tokens, w2v_model, embedding_dim=vector_size, agg=\"sum\", seed=SEED)\n",
    "        for tokens in train_tokens\n",
    "    ])\n",
    "    X_val = np.vstack([\n",
    "        get_tweet_vector(tokens, w2v_model, embedding_dim=vector_size, agg=\"sum\", seed=SEED)\n",
    "        for tokens in val_tokens\n",
    "    ])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    y_train = train_df[\"Label\"].values.astype(np.float32)\n",
    "    y_val = val_df[\"Label\"].values.astype(np.float32)\n",
    "\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128])\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "        torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "    )\n",
    "\n",
    "    g = torch.Generator().manual_seed(SEED)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=g)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    H1 = trial.suggest_int(\"H1\", 340, 500)\n",
    "    H2 = trial.suggest_int(\"H2\", 200, H1)\n",
    "    H3 = trial.suggest_int(\"H3\", 160, H2)\n",
    "    H4 = trial.suggest_int(\"H4\", 120, H3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.15, 0.3)\n",
    "    lr = trial.suggest_float(\"lr\", 8e-5, 3e-4, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"leaky_relu\"])\n",
    "    optimizer_name = \"adamw\"\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    model = FourLayerNet(\n",
    "        D_in=EMBEDDING_DIM,\n",
    "        H1=H1, H2=H2, H3=H3, H4=H4,\n",
    "        dropout=dropout,\n",
    "        activation=activation\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for _ in range(5):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            preds = torch.sigmoid(model(xb)) > 0.5\n",
    "            val_preds.extend(preds.cpu().squeeze().int().numpy())\n",
    "            val_labels.extend(yb.cpu().squeeze().int().numpy())\n",
    "\n",
    "    acc = accuracy_score(val_labels, val_preds)\n",
    "    trial.report(acc, step=0)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    return acc\n",
    "    \n",
    "# pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=2)\n",
    "# study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "# study.optimize(objective, n_trials=40, n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c7b9a1",
   "metadata": {
    "papermill": {
     "duration": 0.007238,
     "end_time": "2025-05-04T14:01:22.777228",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.769990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2 Layers Model - Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cf25ff",
   "metadata": {
    "papermill": {
     "duration": 0.007151,
     "end_time": "2025-05-04T14:01:22.791809",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.784658",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a9c8f62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:01:22.807306Z",
     "iopub.status.busy": "2025-05-04T14:01:22.806912Z",
     "iopub.status.idle": "2025-05-04T14:01:22.811576Z",
     "shell.execute_reply": "2025-05-04T14:01:22.810995Z"
    },
    "papermill": {
     "duration": 0.013668,
     "end_time": "2025-05-04T14:01:22.812677",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.799009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set_seed(42)\n",
    "# params = {\n",
    "#     'window': 7,\n",
    "#     'min_count': 4,\n",
    "#     'batch_size': 64,\n",
    "#     'H1': 375,\n",
    "#     'H2': 374,\n",
    "#     'dropout': 0.27041264306882673,\n",
    "#     'activation': 'leaky_relu',\n",
    "#     'lr': 1.0702e-4,\n",
    "#     'weight_decay': 3.5708e-6,\n",
    "#     'optimizer': 'adamw',\n",
    "#     'vector_size': 300,\n",
    "#     'batch_size' : 64\n",
    "# }\n",
    "\n",
    "\n",
    "# w2v_model = Word2Vec(\n",
    "#     sentences=train_tokens,\n",
    "#     vector_size=params['vector_size'],\n",
    "#     window=params['window'],\n",
    "#     min_count=params['min_count'],\n",
    "#     sg=1,\n",
    "#     workers=1,\n",
    "#     epochs=5,\n",
    "#     seed=SEED\n",
    "# )\n",
    "\n",
    "# X_train = np.vstack([get_tweet_vector(tokens, w2v_model, params['vector_size']) for tokens in train_tokens])\n",
    "# X_val = np.vstack([get_tweet_vector(tokens, w2v_model, params['vector_size']) for tokens in val_tokens])\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_val = scaler.transform(X_val)\n",
    "\n",
    "# y_train = train_df[\"Label\"].values.astype(np.float32)\n",
    "# y_val = val_df[\"Label\"].values.astype(np.float32)\n",
    "\n",
    "# train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train).unsqueeze(1))\n",
    "# val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val).unsqueeze(1))\n",
    "# g = torch.Generator().manual_seed(SEED)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True, generator=g)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "# torch.manual_seed(SEED)\n",
    "# model = TwoLayerNet(\n",
    "#     D_in=params['vector_size'],\n",
    "#     H1=params['H1'],\n",
    "#     H2=params['H2'],\n",
    "#     dropout=params['dropout'],\n",
    "#     activation=params['activation']\n",
    "# ).to(DEVICE)\n",
    "\n",
    "# optimizer_cls = torch.optim.AdamW if params['optimizer'] == 'adamw' else torch.optim.Adam\n",
    "# optimizer = optimizer_cls(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# EPOCHS = 20\n",
    "# train_losses, val_losses = [], []\n",
    "# train_accuracies, val_accuracies = [], []\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     model.train()\n",
    "#     train_preds, train_labels, train_loss_total = [], [], 0\n",
    "#     for xb, yb in train_loader:\n",
    "#         xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "#         optimizer.zero_grad()\n",
    "#         preds = model(xb)\n",
    "#         loss = criterion(preds, yb)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_loss_total += loss.item() * xb.size(0)\n",
    "#         train_preds += (torch.sigmoid(preds) > 0.5).cpu().numpy().astype(int).tolist()\n",
    "#         train_labels += yb.cpu().numpy().astype(int).tolist()\n",
    "#     train_losses.append(train_loss_total / len(train_loader.dataset))\n",
    "#     train_accuracies.append(accuracy_score(train_labels, train_preds))\n",
    "\n",
    "#     model.eval()\n",
    "#     val_preds, val_labels, val_loss_total = [], [], 0\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in val_loader:\n",
    "#             xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "#             preds = model(xb)\n",
    "#             loss = criterion(preds, yb)\n",
    "#             val_loss_total += loss.item() * xb.size(0)\n",
    "#             val_preds += (torch.sigmoid(preds) > 0.5).cpu().numpy().astype(int).tolist()\n",
    "#             val_labels += yb.cpu().numpy().astype(int).tolist()\n",
    "#     val_losses.append(val_loss_total / len(val_loader.dataset))\n",
    "#     val_accuracies.append(accuracy_score(val_labels, val_preds))\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS} | Train Acc: {train_accuracies[-1]:.4f}, Val Acc: {val_accuracies[-1]:.4f}\")\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(epochs, train_losses, label='Train Loss', color='blue', linewidth=2)\n",
    "# plt.plot(epochs, val_losses, label='Validation Loss', color='green', linewidth=2)\n",
    "# plt.plot(epochs, train_accuracies, label='Train Accuracy', color='orange', linewidth=2)\n",
    "# plt.plot(epochs, val_accuracies, label='Validation Accuracy', color='red', linewidth=2)\n",
    "\n",
    "# plt.title('Learning Curves', fontsize=16)\n",
    "# plt.xlabel('Epoch', fontsize=14)\n",
    "# plt.ylabel('Loss / Accuracy', fontsize=14)\n",
    "# plt.xticks(ticks=range(0, EPOCHS + 1, 5))\n",
    "# plt.yticks(fontsize=12)\n",
    "# plt.ylim(0.35, 0.9)\n",
    "# plt.grid(True, linestyle='--', linewidth=0.6)\n",
    "\n",
    "# plt.legend(\n",
    "#     loc='center left',\n",
    "#     bbox_to_anchor=(1, 0.5),\n",
    "#     fontsize=12,\n",
    "#     frameon=False\n",
    "# )\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"learning_curves_2layer.png\", dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ed5e66",
   "metadata": {
    "papermill": {
     "duration": 0.007358,
     "end_time": "2025-05-04T14:01:22.827289",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.819931",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Confusion Matrix - ROC - Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74dc142e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:01:22.843013Z",
     "iopub.status.busy": "2025-05-04T14:01:22.842839Z",
     "iopub.status.idle": "2025-05-04T14:01:22.846633Z",
     "shell.execute_reply": "2025-05-04T14:01:22.846100Z"
    },
    "papermill": {
     "duration": 0.012891,
     "end_time": "2025-05-04T14:01:22.847641",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.834750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set_seed(42)\n",
    "# EPOCHS = 50\n",
    "# PATIENCE = 5\n",
    "# best_val_loss = float(\"inf\")\n",
    "# patience_counter = 0\n",
    "\n",
    "# model = TwoLayerNet(\n",
    "#     D_in=300,\n",
    "#     H1=375,\n",
    "#     H2=374,\n",
    "#     dropout=0.2704,\n",
    "#     activation='leaky_relu'\n",
    "# ).to(DEVICE)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1.07e-4, weight_decay=3.57e-6)\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, generator=torch.Generator().manual_seed(SEED))\n",
    "# val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# train_losses, val_losses = [], []\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for xb, yb in train_loader:\n",
    "#         xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "#         optimizer.zero_grad()\n",
    "#         preds = model(xb)\n",
    "#         loss = criterion(preds, yb)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "#     train_losses.append(running_loss / len(train_loader))\n",
    "\n",
    "#     model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     all_probs, all_labels = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in val_loader:\n",
    "#             xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "#             logits = model(xb)\n",
    "#             val_loss += criterion(logits, yb).item()\n",
    "#             all_probs.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "#             all_labels.extend(yb.cpu().numpy())\n",
    "\n",
    "#     val_losses.append(val_loss / len(val_loader))\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_losses[-1]:.4f} | Val Loss: {val_losses[-1]:.4f}\")\n",
    "\n",
    "#     if val_losses[-1] < best_val_loss:\n",
    "#         best_val_loss = val_losses[-1]\n",
    "#         patience_counter = 0\n",
    "#         torch.save(model.state_dict(), \"best_2layer_model.pt\")\n",
    "#     else:\n",
    "#         patience_counter += 1\n",
    "#         if patience_counter >= PATIENCE:\n",
    "#             print(\"Early stopping triggered.\")\n",
    "#             break\n",
    "\n",
    "# model.load_state_dict(torch.load(\"best_2layer_model.pt\"))\n",
    "\n",
    "# model.eval()\n",
    "# all_probs, all_labels = [], []\n",
    "# with torch.no_grad():\n",
    "#     for xb, yb in val_loader:\n",
    "#         xb = xb.to(DEVICE)\n",
    "#         probs = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "#         all_probs.extend(probs)\n",
    "#         all_labels.extend(yb.numpy())\n",
    "\n",
    "# y_true = np.array(all_labels).astype(int).flatten()\n",
    "# y_scores = np.array(all_probs).flatten()\n",
    "# y_pred = (y_scores >= 0.5).astype(int)\n",
    "\n",
    "# fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# plt.figure(figsize=(6, 6))\n",
    "# plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.4f}\", color=\"darkorange\", lw=2)\n",
    "# plt.plot([0, 1], [0, 1], \"k--\", lw=1)\n",
    "# plt.xlabel(\"False Positive Rate\")\n",
    "# plt.ylabel(\"True Positive Rate\")\n",
    "# plt.title(\"ROC Curve (2-layer MLP)\")\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"roc_curve_2layer.png\", dpi=300)\n",
    "# plt.show()\n",
    "\n",
    "# cm = confusion_matrix(y_true, y_pred)\n",
    "# plt.figure(figsize=(5, 4))\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "# plt.xlabel(\"Predicted Label\")\n",
    "# plt.ylabel(\"True Label\")\n",
    "# plt.title(\"Confusion Matrix (2-layer MLP)\")\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"confusion_matrix_2layer.png\", dpi=300)\n",
    "# plt.show()\n",
    "\n",
    "# print(\"Classification Report (2-layer MLP):\")\n",
    "# print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa184612",
   "metadata": {
    "papermill": {
     "duration": 0.007226,
     "end_time": "2025-05-04T14:01:22.862318",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.855092",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3 Layers Model - Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54002b75",
   "metadata": {
    "papermill": {
     "duration": 0.007268,
     "end_time": "2025-05-04T14:01:22.876939",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.869671",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f06665ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:01:22.892624Z",
     "iopub.status.busy": "2025-05-04T14:01:22.892413Z",
     "iopub.status.idle": "2025-05-04T14:01:22.896377Z",
     "shell.execute_reply": "2025-05-04T14:01:22.895908Z"
    },
    "papermill": {
     "duration": 0.012913,
     "end_time": "2025-05-04T14:01:22.897405",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.884492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set_seed(42)\n",
    "# params = {\n",
    "#     'vector_size': 300,\n",
    "#     'window': 7,\n",
    "#     'min_count': 3,\n",
    "#     'batch_size': 64,\n",
    "#     'H1': 476,\n",
    "#     'H2': 362,\n",
    "#     'H3': 293,\n",
    "#     'dropout': 0.191,\n",
    "#     'activation': 'relu',\n",
    "#     'lr': 2.698e-4,\n",
    "#     'weight_decay': 2.351e-6,\n",
    "#     'optimizer': 'adamw'\n",
    "# }\n",
    "\n",
    "\n",
    "# w2v_model = Word2Vec(\n",
    "#     sentences=train_tokens,\n",
    "#     vector_size=params['vector_size'],\n",
    "#     window=params['window'],\n",
    "#     min_count=params['min_count'],\n",
    "#     sg=1,\n",
    "#     workers=1,\n",
    "#     epochs=5,\n",
    "#     seed=SEED\n",
    "# )\n",
    "\n",
    "# X_train = np.vstack([get_tweet_vector(tokens, w2v_model, params['vector_size']) for tokens in train_tokens])\n",
    "# X_val = np.vstack([get_tweet_vector(tokens, w2v_model, params['vector_size']) for tokens in val_tokens])\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_val = scaler.transform(X_val)\n",
    "\n",
    "# y_train = train_df[\"Label\"].values.astype(np.float32)\n",
    "# y_val = val_df[\"Label\"].values.astype(np.float32)\n",
    "\n",
    "# train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train).unsqueeze(1))\n",
    "# val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val).unsqueeze(1))\n",
    "# g = torch.Generator().manual_seed(SEED)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True, generator=g)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "# torch.manual_seed(SEED)\n",
    "# model = ThreeLayerNet(\n",
    "#     D_in=params['vector_size'],\n",
    "#     H1=params['H1'],\n",
    "#     H2=params['H2'],\n",
    "#     H3=params['H3'],\n",
    "#     dropout=params['dropout'],\n",
    "#     activation=params['activation']\n",
    "# ).to(DEVICE)\n",
    "\n",
    "# optimizer_cls = torch.optim.AdamW if params['optimizer'] == 'adamw' else torch.optim.Adam\n",
    "# optimizer = optimizer_cls(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# EPOCHS = 20\n",
    "# train_losses, val_losses = [], []\n",
    "# train_accuracies, val_accuracies = [], []\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     model.train()\n",
    "#     train_preds, train_labels, train_loss_total = [], [], 0\n",
    "#     for xb, yb in train_loader:\n",
    "#         xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "#         optimizer.zero_grad()\n",
    "#         preds = model(xb)\n",
    "#         loss = criterion(preds, yb)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_loss_total += loss.item() * xb.size(0)\n",
    "#         train_preds += (torch.sigmoid(preds) > 0.5).cpu().numpy().astype(int).tolist()\n",
    "#         train_labels += yb.cpu().numpy().astype(int).tolist()\n",
    "#     train_losses.append(train_loss_total / len(train_loader.dataset))\n",
    "#     train_accuracies.append(accuracy_score(train_labels, train_preds))\n",
    "\n",
    "#     model.eval()\n",
    "#     val_preds, val_labels, val_loss_total = [], [], 0\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in val_loader:\n",
    "#             xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "#             preds = model(xb)\n",
    "#             loss = criterion(preds, yb)\n",
    "#             val_loss_total += loss.item() * xb.size(0)\n",
    "#             val_preds += (torch.sigmoid(preds) > 0.5).cpu().numpy().astype(int).tolist()\n",
    "#             val_labels += yb.cpu().numpy().astype(int).tolist()\n",
    "#     val_losses.append(val_loss_total / len(val_loader.dataset))\n",
    "#     val_accuracies.append(accuracy_score(val_labels, val_preds))\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS} | Train Acc: {train_accuracies[-1]:.4f}, Val Acc: {val_accuracies[-1]:.4f}\")\n",
    "\n",
    "# epochs = list(range(1, EPOCHS + 1))\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(epochs, train_losses, label='Train Loss', color='blue', linewidth=2)\n",
    "# plt.plot(epochs, val_losses, label='Validation Loss', color='green', linewidth=2)\n",
    "# plt.plot(epochs, train_accuracies, label='Train Accuracy', color='orange', linewidth=2)\n",
    "# plt.plot(epochs, val_accuracies, label='Validation Accuracy', color='red', linewidth=2)\n",
    "# plt.title('3-Layer MLP Learning Curves', fontsize=16)\n",
    "# plt.xlabel('Epoch', fontsize=14)\n",
    "# plt.ylabel('Loss / Accuracy', fontsize=14)\n",
    "# plt.xticks(ticks=range(0, EPOCHS + 1, 5))\n",
    "# plt.yticks(fontsize=12)\n",
    "# plt.ylim(0.2, 0.9)\n",
    "# plt.grid(True, linestyle='--', linewidth=0.6)\n",
    "# plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=12, frameon=False)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"learning_curves_3layer.png\", dpi=300, bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5fc43c",
   "metadata": {
    "papermill": {
     "duration": 0.007186,
     "end_time": "2025-05-04T14:01:22.911928",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.904742",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Confusion Matrix - ROC - Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89cf2419",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:01:22.928050Z",
     "iopub.status.busy": "2025-05-04T14:01:22.927824Z",
     "iopub.status.idle": "2025-05-04T14:01:22.932179Z",
     "shell.execute_reply": "2025-05-04T14:01:22.931518Z"
    },
    "papermill": {
     "duration": 0.014005,
     "end_time": "2025-05-04T14:01:22.933375",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.919370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set_seed(42)\n",
    "\n",
    "# EPOCHS = 50\n",
    "# PATIENCE = 5\n",
    "# best_val_loss = float(\"inf\")\n",
    "# patience_counter = 0\n",
    "\n",
    "# model = ThreeLayerNet(\n",
    "#     D_in=300,\n",
    "#     H1=453,\n",
    "#     H2=395,\n",
    "#     H3=302,\n",
    "#     dropout=0.1602,\n",
    "#     activation='relu'\n",
    "# ).to(DEVICE)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1.1958e-4, weight_decay=6.7527e-6)\n",
    "# criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, generator=torch.Generator().manual_seed(SEED))\n",
    "# val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# train_losses, val_losses = [], []\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for xb, yb in train_loader:\n",
    "#         xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "#         optimizer.zero_grad()\n",
    "#         preds = model(xb)\n",
    "#         loss = criterion(preds, yb)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "#     train_losses.append(running_loss / len(train_loader))\n",
    "\n",
    "#     model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     all_probs, all_labels = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in val_loader:\n",
    "#             xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "#             logits = model(xb)\n",
    "#             val_loss += criterion(logits, yb).item()\n",
    "#             all_probs.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "#             all_labels.extend(yb.cpu().numpy())\n",
    "#     val_losses.append(val_loss / len(val_loader))\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_losses[-1]:.4f} | Val Loss: {val_losses[-1]:.4f}\")\n",
    "\n",
    "#     if val_losses[-1] < best_val_loss:\n",
    "#         best_val_loss = val_losses[-1]\n",
    "#         patience_counter = 0\n",
    "#         torch.save(model.state_dict(), \"best_3layer_model.pt\")\n",
    "#     else:\n",
    "#         patience_counter += 1\n",
    "#         if patience_counter >= PATIENCE:\n",
    "#             print(\"Early stopping triggered.\")\n",
    "#             break\n",
    "\n",
    "# model.load_state_dict(torch.load(\"best_3layer_model.pt\"))\n",
    "\n",
    "# model.eval()\n",
    "# all_probs, all_labels = [], []\n",
    "# with torch.no_grad():\n",
    "#     for xb, yb in val_loader:\n",
    "#         xb = xb.to(DEVICE)\n",
    "#         probs = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "#         all_probs.extend(probs)\n",
    "#         all_labels.extend(yb.numpy())\n",
    "\n",
    "# y_true = np.array(all_labels).astype(int).flatten()\n",
    "# y_scores = np.array(all_probs).flatten()\n",
    "# y_pred = (y_scores >= 0.5).astype(int)\n",
    "\n",
    "# fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# plt.figure(figsize=(6, 6))\n",
    "# plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.4f}\", color=\"darkorange\", lw=2)\n",
    "# plt.plot([0, 1], [0, 1], \"k--\", lw=1)\n",
    "# plt.xlabel(\"False Positive Rate\")\n",
    "# plt.ylabel(\"True Positive Rate\")\n",
    "# plt.title(\"ROC Curve (3-layer MLP)\")\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"roc_curve_3layer.png\", dpi=300)\n",
    "# plt.show()\n",
    "\n",
    "# cm = confusion_matrix(y_true, y_pred)\n",
    "# plt.figure(figsize=(5, 4))\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "# plt.xlabel(\"Predicted Label\")\n",
    "# plt.ylabel(\"True Label\")\n",
    "# plt.title(\"Confusion Matrix (3-layer MLP)\")\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"confusion_matrix_3layer.png\", dpi=300)\n",
    "# plt.show()\n",
    "\n",
    "# print(\"Classification Report (3-layer MLP):\")\n",
    "# print(classification_report(y_true, y_pred, digits=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95abb71",
   "metadata": {
    "papermill": {
     "duration": 0.007745,
     "end_time": "2025-05-04T14:01:22.948936",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.941191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4 Layers Model - Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db96c809",
   "metadata": {
    "papermill": {
     "duration": 0.007477,
     "end_time": "2025-05-04T14:01:22.964292",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.956815",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb93ca55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:01:22.980427Z",
     "iopub.status.busy": "2025-05-04T14:01:22.980211Z",
     "iopub.status.idle": "2025-05-04T14:01:22.985052Z",
     "shell.execute_reply": "2025-05-04T14:01:22.984335Z"
    },
    "papermill": {
     "duration": 0.014251,
     "end_time": "2025-05-04T14:01:22.986087",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.971836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set_seed(42)\n",
    "# params = {\n",
    "#     'window': 7,\n",
    "#     'min_count': 4,\n",
    "#     'batch_size': 64,\n",
    "#     'H1': 387,\n",
    "#     'H2': 359,\n",
    "#     'H3': 320,\n",
    "#     'H4': 257,\n",
    "#     'dropout': 0.1775,\n",
    "#     'activation': 'relu',\n",
    "#     'lr': 1.3767e-4,\n",
    "#     'weight_decay': 1.0213e-4,\n",
    "#     'optimizer': 'adamw',\n",
    "#     'vector_size': 300\n",
    "# }\n",
    "\n",
    "\n",
    "# w2v_model = Word2Vec(\n",
    "#     sentences=train_tokens,\n",
    "#     vector_size=params['vector_size'],\n",
    "#     window=params['window'],\n",
    "#     min_count=params['min_count'],\n",
    "#     sg=1,\n",
    "#     workers=1,\n",
    "#     epochs=5,\n",
    "#     seed=SEED\n",
    "# )\n",
    "\n",
    "# X_train = np.vstack([get_tweet_vector(tokens, w2v_model, params['vector_size']) for tokens in train_tokens])\n",
    "# X_val = np.vstack([get_tweet_vector(tokens, w2v_model, params['vector_size']) for tokens in val_tokens])\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_val = scaler.transform(X_val)\n",
    "\n",
    "# y_train = train_df[\"Label\"].values.astype(np.float32)\n",
    "# y_val = val_df[\"Label\"].values.astype(np.float32)\n",
    "\n",
    "# train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train).unsqueeze(1))\n",
    "# val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val).unsqueeze(1))\n",
    "# g = torch.Generator().manual_seed(SEED)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True, generator=g)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "# model = FourLayerNet(\n",
    "#     D_in=params['vector_size'],\n",
    "#     H1=params['H1'],\n",
    "#     H2=params['H2'],\n",
    "#     H3=params['H3'],\n",
    "#     H4=params['H4'],\n",
    "#     dropout=params['dropout'],\n",
    "#     activation=params['activation']\n",
    "# ).to(DEVICE)\n",
    "\n",
    "# optimizer_cls = torch.optim.AdamW if params['optimizer'] == 'adamw' else torch.optim.Adam\n",
    "# optimizer = optimizer_cls(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# EPOCHS = 20\n",
    "# train_losses, val_losses = [], []\n",
    "# train_accuracies, val_accuracies = [], []\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     model.train()\n",
    "#     train_preds, train_labels, train_loss_total = [], [], 0\n",
    "#     for xb, yb in train_loader:\n",
    "#         xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "#         optimizer.zero_grad()\n",
    "#         preds = model(xb)\n",
    "#         loss = criterion(preds, yb)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_loss_total += loss.item() * xb.size(0)\n",
    "#         train_preds += (torch.sigmoid(preds) > 0.5).cpu().numpy().astype(int).tolist()\n",
    "#         train_labels += yb.cpu().numpy().astype(int).tolist()\n",
    "#     train_losses.append(train_loss_total / len(train_loader.dataset))\n",
    "#     train_accuracies.append(accuracy_score(train_labels, train_preds))\n",
    "\n",
    "#     model.eval()\n",
    "#     val_preds, val_labels, val_loss_total = [], [], 0\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in val_loader:\n",
    "#             xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "#             preds = model(xb)\n",
    "#             loss = criterion(preds, yb)\n",
    "#             val_loss_total += loss.item() * xb.size(0)\n",
    "#             val_preds += (torch.sigmoid(preds) > 0.5).cpu().numpy().astype(int).tolist()\n",
    "#             val_labels += yb.cpu().numpy().astype(int).tolist()\n",
    "#     val_losses.append(val_loss_total / len(val_loader.dataset))\n",
    "#     val_accuracies.append(accuracy_score(val_labels, val_preds))\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS} | Train Acc: {train_accuracies[-1]:.4f}, Val Acc: {val_accuracies[-1]:.4f}\")\n",
    "\n",
    "# epochs = list(range(1, EPOCHS + 1))\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(epochs, train_losses, label='Train Loss', color='blue', linewidth=2)\n",
    "# plt.plot(epochs, val_losses, label='Validation Loss', color='green', linewidth=2)\n",
    "# plt.plot(epochs, train_accuracies, label='Train Accuracy', color='orange', linewidth=2)\n",
    "# plt.plot(epochs, val_accuracies, label='Validation Accuracy', color='red', linewidth=2)\n",
    "\n",
    "# plt.title('Learning Curves (4-layer MLP)', fontsize=16)\n",
    "# plt.xlabel('Epoch', fontsize=14)\n",
    "# plt.ylabel('Loss / Accuracy', fontsize=14)\n",
    "# plt.xticks(ticks=range(0, EPOCHS + 1, 5))\n",
    "# plt.yticks(fontsize=12)\n",
    "# plt.ylim(0.3, 0.9)\n",
    "# plt.grid(True, linestyle='--', linewidth=0.6)\n",
    "\n",
    "# plt.legend(\n",
    "#     loc='center left',\n",
    "#     bbox_to_anchor=(1, 0.5),\n",
    "#     fontsize=12,\n",
    "#     frameon=False\n",
    "# )\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"learning_curves_4layer.png\", dpi=300, bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad77f20",
   "metadata": {
    "papermill": {
     "duration": 0.007287,
     "end_time": "2025-05-04T14:01:23.000800",
     "exception": false,
     "start_time": "2025-05-04T14:01:22.993513",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Confusion Matrix - ROC - Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b234a738",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:01:23.016228Z",
     "iopub.status.busy": "2025-05-04T14:01:23.016035Z",
     "iopub.status.idle": "2025-05-04T14:01:23.019905Z",
     "shell.execute_reply": "2025-05-04T14:01:23.019429Z"
    },
    "papermill": {
     "duration": 0.012963,
     "end_time": "2025-05-04T14:01:23.020924",
     "exception": false,
     "start_time": "2025-05-04T14:01:23.007961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set_seed(42)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True, generator=torch.Generator().manual_seed(SEED))\n",
    "# val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "# torch.manual_seed(SEED)\n",
    "# model = FourLayerNet(\n",
    "#     D_in=params['vector_size'],\n",
    "#     H1=params['H1'],\n",
    "#     H2=params['H2'],\n",
    "#     H3=params['H3'],\n",
    "#     H4=params['H4'],\n",
    "#     dropout=params['dropout'],\n",
    "#     activation=params['activation']\n",
    "# ).to(DEVICE)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# EPOCHS = 50\n",
    "# PATIENCE = 5\n",
    "# best_val_loss = float(\"inf\")\n",
    "# patience_counter = 0\n",
    "\n",
    "# train_losses, val_losses = [], []\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     model.train()\n",
    "#     total_train_loss = 0\n",
    "#     for xb, yb in train_loader:\n",
    "#         xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "#         optimizer.zero_grad()\n",
    "#         preds = model(xb)\n",
    "#         loss = criterion(preds, yb)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_train_loss += loss.item()\n",
    "#     train_losses.append(total_train_loss / len(train_loader))\n",
    "\n",
    "#     model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     all_probs, all_labels = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in val_loader:\n",
    "#             xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "#             logits = model(xb)\n",
    "#             val_loss += criterion(logits, yb).item()\n",
    "#             all_probs.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "#             all_labels.extend(yb.cpu().numpy())\n",
    "\n",
    "#     val_losses.append(val_loss / len(val_loader))\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_losses[-1]:.4f} | Val Loss: {val_losses[-1]:.4f}\")\n",
    "\n",
    "#     if val_losses[-1] < best_val_loss:\n",
    "#         best_val_loss = val_losses[-1]\n",
    "#         patience_counter = 0\n",
    "#         torch.save(model.state_dict(), \"best_4layer_model.pt\")\n",
    "#     else:\n",
    "#         patience_counter += 1\n",
    "#         if patience_counter >= PATIENCE:\n",
    "#             print(\"Early stopping triggered.\")\n",
    "#             break\n",
    "\n",
    "# model.load_state_dict(torch.load(\"best_4layer_model.pt\"))\n",
    "# model.eval()\n",
    "\n",
    "# all_probs, all_labels = [], []\n",
    "# with torch.no_grad():\n",
    "#     for xb, yb in val_loader:\n",
    "#         xb = xb.to(DEVICE)\n",
    "#         probs = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "#         all_probs.extend(probs)\n",
    "#         all_labels.extend(yb.numpy())\n",
    "\n",
    "# y_true = np.array(all_labels).astype(int).flatten()\n",
    "# y_scores = np.array(all_probs).flatten()\n",
    "# y_pred = (y_scores >= 0.5).astype(int)\n",
    "\n",
    "# fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# plt.figure(figsize=(6, 6))\n",
    "# plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.4f}\", color=\"darkorange\", lw=2)\n",
    "# plt.plot([0, 1], [0, 1], \"k--\", lw=1)\n",
    "# plt.xlabel(\"False Positive Rate\")\n",
    "# plt.ylabel(\"True Positive Rate\")\n",
    "# plt.title(\"ROC Curve (4-layer MLP)\")\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"roc_curve_4layer.png\", dpi=300)\n",
    "# plt.show()\n",
    "\n",
    "# cm = confusion_matrix(y_true, y_pred)\n",
    "# plt.figure(figsize=(5, 4))\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "# plt.xlabel(\"Predicted Label\")\n",
    "# plt.ylabel(\"True Label\")\n",
    "# plt.title(\"Confusion Matrix (4-layer MLP)\")\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"confusion_matrix_4layer.png\", dpi=300)\n",
    "# plt.show()\n",
    "\n",
    "# print(\"Classification Report (4-layer MLP):\")\n",
    "# print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ba7ad1",
   "metadata": {
    "papermill": {
     "duration": 0.007108,
     "end_time": "2025-05-04T14:01:23.035334",
     "exception": false,
     "start_time": "2025-05-04T14:01:23.028226",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Best Model - Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "313cbae3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:01:23.050820Z",
     "iopub.status.busy": "2025-05-04T14:01:23.050640Z",
     "iopub.status.idle": "2025-05-04T14:04:52.326375Z",
     "shell.execute_reply": "2025-05-04T14:04:52.325512Z"
    },
    "papermill": {
     "duration": 209.285036,
     "end_time": "2025-05-04T14:04:52.327690",
     "exception": false,
     "start_time": "2025-05-04T14:01:23.042654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 0.4870 | Val Loss: 0.4612\n",
      "Epoch 2/50 | Train Loss: 0.4626 | Val Loss: 0.4527\n",
      "Epoch 3/50 | Train Loss: 0.4544 | Val Loss: 0.4494\n",
      "Epoch 4/50 | Train Loss: 0.4484 | Val Loss: 0.4470\n",
      "Epoch 5/50 | Train Loss: 0.4437 | Val Loss: 0.4468\n",
      "Epoch 6/50 | Train Loss: 0.4400 | Val Loss: 0.4445\n",
      "Epoch 7/50 | Train Loss: 0.4357 | Val Loss: 0.4476\n",
      "Epoch 8/50 | Train Loss: 0.4319 | Val Loss: 0.4424\n",
      "Epoch 9/50 | Train Loss: 0.4295 | Val Loss: 0.4411\n",
      "Epoch 10/50 | Train Loss: 0.4263 | Val Loss: 0.4419\n",
      "Epoch 11/50 | Train Loss: 0.4229 | Val Loss: 0.4407\n",
      "Epoch 12/50 | Train Loss: 0.4201 | Val Loss: 0.4419\n",
      "Epoch 13/50 | Train Loss: 0.4174 | Val Loss: 0.4413\n",
      "Epoch 14/50 | Train Loss: 0.4148 | Val Loss: 0.4422\n",
      "Epoch 15/50 | Train Loss: 0.4125 | Val Loss: 0.4423\n",
      "Epoch 16/50 | Train Loss: 0.4096 | Val Loss: 0.4419\n",
      "Early stopping triggered.\n",
      "✅ Training complete. Best model saved as best_2layer_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/1321597316.py:111: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_2layer_model.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ submission.csv saved and ready for upload!\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Parameters ---\n",
    "params = {\n",
    "    'vector_size': 300,\n",
    "    'window': 7,\n",
    "    'min_count': 4,\n",
    "    'batch_size': 64,\n",
    "    'H1': 375,\n",
    "    'H2': 374,\n",
    "    'dropout': 0.2704,\n",
    "    'activation': 'leaky_relu',\n",
    "    'lr': 1.0702e-4,\n",
    "    'weight_decay': 3.5708e-6,\n",
    "    'optimizer': 'adamw'\n",
    "}\n",
    "\n",
    "# --- Train Word2Vec ---\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=train_tokens,\n",
    "    vector_size=params['vector_size'],\n",
    "    window=params['window'],\n",
    "    min_count=params['min_count'],\n",
    "    sg=1,\n",
    "    workers=1,\n",
    "    epochs=5,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "X_train = np.vstack([get_tweet_vector(tokens, w2v_model, params['vector_size']) for tokens in train_tokens])\n",
    "X_val = np.vstack([get_tweet_vector(tokens, w2v_model, params['vector_size']) for tokens in val_tokens])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "y_train = train_df[\"Label\"].values.astype(np.float32)\n",
    "y_val = val_df[\"Label\"].values.astype(np.float32)\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train).unsqueeze(1))\n",
    "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val).unsqueeze(1))\n",
    "\n",
    "g = torch.Generator().manual_seed(SEED)\n",
    "train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True, generator=g)\n",
    "val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "X_test = np.vstack([\n",
    "    get_tweet_vector(tokens, w2v_model, params['vector_size']) for tokens in test_tokens\n",
    "])\n",
    "X_test = scaler.transform(X_test)  # Use the same scaler as train/val\n",
    "\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32))\n",
    "test_loader = DataLoader(test_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "model = TwoLayerNet(\n",
    "    D_in=params['vector_size'],\n",
    "    H1=params['H1'],\n",
    "    H2=params['H2'],\n",
    "    dropout=params['dropout'],\n",
    "    activation=params['activation']\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "EPOCHS = 50\n",
    "PATIENCE = 5\n",
    "best_val_loss = float(\"inf\")\n",
    "patience_counter = 0\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    train_losses.append(epoch_loss / len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            val_loss += loss.item()\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_losses[-1]:.4f} | Val Loss: {val_losses[-1]:.4f}\")\n",
    "\n",
    "    if val_losses[-1] < best_val_loss:\n",
    "        best_val_loss = val_losses[-1]\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_2layer_model.pt\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(\"✅ Training complete. Best model saved as best_2layer_model.pt\")\n",
    "model.load_state_dict(torch.load(\"best_2layer_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for (xb,) in test_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy().flatten()\n",
    "        preds.extend((probs >= 0.5).astype(int))\n",
    "\n",
    "# --- Save submission ---\n",
    "submission_df = pd.DataFrame({\n",
    "    \"ID\": test_df[\"ID\"],\n",
    "    \"Label\": preds\n",
    "})\n",
    "submission_df.to_csv(\"/kaggle/working/submission.csv\", index=False)\n",
    "print(\"✅ submission.csv saved and ready for upload!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11446837,
     "sourceId": 96317,
     "sourceType": "competition"
    },
    {
     "datasetId": 6763,
     "sourceId": 9801,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 14010,
     "sourceId": 18875,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 362.638404,
   "end_time": "2025-05-04T14:04:55.428093",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-04T13:58:52.789689",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
